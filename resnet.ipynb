{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD, Adam\n",
    "from keras.applications import *\n",
    "from keras.callbacks import *\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "import numpy as np\n",
    "import shap\n",
    "import keras.backend as K\n",
    "import json\n",
    "import split_folders\n",
    "import urllib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_folders.ratio('data/', output=\"output\", seed=1337, ratio=(.8, .2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('resnet50_weights_tf.h5', <http.client.HTTPMessage at 0x7f412c199860>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels.h5\", \"resnet50_weights_tf.h5\")                     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = 244"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained model and choose two images to explain\n",
    "model = VGG16(weights='imagenet', include_top=True) ## CHANGE INCLUDE TOP TO FALSE\n",
    "X,y = shap.datasets.imagenet50()\n",
    "to_explain = X[[6,7]]\n",
    "\n",
    "# load the ImageNet class names\n",
    "url = \"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\"\n",
    "fname = shap.datasets.cache(url)\n",
    "with open(fname) as f:\n",
    "    class_names = json.load(f)\n",
    "\n",
    "# explain how the input to the 7th layer of the model explains the top two classes\n",
    "def map2layer(x, layer):\n",
    "    feed_dict = dict(zip([model.layers[0].input], [preprocess_input(x.copy())]))\n",
    "    return K.get_session().run(model.layers[layer].input, feed_dict)\n",
    "e = shap.GradientExplainer((model.layers[7].input, model.layers[-1].output), map2layer(preprocess_input(X.copy()), 7))\n",
    "shap_values,indexes = e.shap_values(map2layer(to_explain, 7), ranked_outputs=2)\n",
    "\n",
    "# get the names for the classes\n",
    "index_names = np.vectorize(lambda x: class_names[str(x)][1])(indexes)\n",
    "\n",
    "# plot the explanations\n",
    "shap.image_plot(shap_values, to_explain, index_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.image_plot(shap_values, to_explain, index_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3065 images belonging to 14 classes.\n",
      "Found 774 images belonging to 14 classes.\n"
     ]
    }
   ],
   "source": [
    "img_width = target_size\n",
    "img_height = target_size\n",
    "batch_size =30\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling (we do not want to modify the testing data)\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "\n",
    "# The generator object. \n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'output/train',\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    'output/val',\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3065 images belonging to 14 classes.\n",
      "Found 774 images belonging to 14 classes.\n"
     ]
    }
   ],
   "source": [
    "base_model = resnet50.ResNet50(include_top=False, weights='imagenet',\n",
    "                       input_tensor=None, input_shape=(target_size,target_size,3), pooling=None, classes=14)\n",
    "\n",
    "TRAIN_DIR = \"output/train\"\n",
    "TEST_DIR = \"output/val\"\n",
    "HEIGHT = target_size\n",
    "WIDTH = target_size\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "train_datagen =  ImageDataGenerator(\n",
    "      preprocessing_function=preprocess_input,\n",
    "      rotation_range=90,\n",
    "      horizontal_flip=True,\n",
    "      vertical_flip=True,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2\n",
    "    )\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(TRAIN_DIR, \n",
    "                                                    target_size=(HEIGHT, WIDTH), \n",
    "                                                    batch_size=BATCH_SIZE)\n",
    "\n",
    "test_datagen =  ImageDataGenerator(\n",
    "      preprocessing_function=preprocess_input,\n",
    "      rotation_range=90,\n",
    "      horizontal_flip=True,\n",
    "      vertical_flip=True,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2\n",
    "    )\n",
    "\n",
    "test_generator = train_datagen.flow_from_directory(TEST_DIR, \n",
    "                                                    target_size=(HEIGHT, WIDTH), \n",
    "                                                    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_finetune_model(base_model, dropout, fc_layers, num_classes):\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    for fc in fc_layers:\n",
    "        # New FC layer, random init\n",
    "        x = Dense(fc, activation='relu')(x) \n",
    "        x = Dropout(dropout)(x)\n",
    "\n",
    "    # New softmax layer\n",
    "    predictions = Dense(num_classes, activation='softmax')(x) \n",
    "    \n",
    "    finetune_model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    return finetune_model\n",
    "\n",
    "class_list = train_generator.class_indices\n",
    "FC_LAYERS = [256]\n",
    "dropout = 0.5\n",
    "\n",
    "finetune_model = build_finetune_model(base_model, \n",
    "                                      dropout=dropout, \n",
    "                                      fc_layers=FC_LAYERS, \n",
    "                                      num_classes=len(class_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('checkpoints'):\n",
    "        os.mkdir('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/PIL/Image.py:931: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 26/102 [======>.......................] - ETA: 25s - loss: 0.9191 - acc: 0.7295"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:747: UserWarning: Possibly corrupt EXIF data.  Expecting to read 6 bytes but only got 5. Skipping tag 42036\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 19s 191ms/step - loss: 0.9463 - acc: 0.7105 - val_loss: 0.9012 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00001: saving model to ./checkpoints/ResNet50_model_weights.h5\n",
      "Epoch 2/5\n",
      "102/102 [==============================] - 13s 131ms/step - loss: 0.9121 - acc: 0.7049 - val_loss: 0.8669 - val_acc: 0.6967\n",
      "\n",
      "Epoch 00002: saving model to ./checkpoints/ResNet50_model_weights.h5\n",
      "Epoch 3/5\n",
      "102/102 [==============================] - 13s 124ms/step - loss: 0.8593 - acc: 0.7255 - val_loss: 0.6765 - val_acc: 0.7755\n",
      "\n",
      "Epoch 00003: saving model to ./checkpoints/ResNet50_model_weights.h5\n",
      "Epoch 4/5\n",
      "102/102 [==============================] - 13s 123ms/step - loss: 0.8239 - acc: 0.7310 - val_loss: 0.8098 - val_acc: 0.7733\n",
      "\n",
      "Epoch 00004: saving model to ./checkpoints/ResNet50_model_weights.h5\n",
      "Epoch 5/5\n",
      "102/102 [==============================] - 13s 124ms/step - loss: 0.8136 - acc: 0.7408 - val_loss: 0.6610 - val_acc: 0.8067\n",
      "\n",
      "Epoch 00005: saving model to ./checkpoints/ResNet50_model_weights.h5\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 30\n",
    "num_train_images = 3065\n",
    "\n",
    "adam = Adam(lr=0.00001) # was 0.00001\n",
    "finetune_model.compile(adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "filepath=\"./checkpoints/\" + \"ResNet50\" + \"_model_weights.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor=[\"acc\"], verbose=1, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history = finetune_model.fit_generator(train_generator, epochs=NUM_EPOCHS, workers=8, \n",
    "                                       steps_per_epoch=num_train_images // BATCH_SIZE, \n",
    "                                       shuffle=True, callbacks=callbacks_list, validation_data = test_generator,\n",
    "                                      validation_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 244, 244, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = load_img('output/val/boxer/1485179194-boxer-dog-portrait.jpg',target_size=(target_size,target_size) )\n",
    "img = img_to_array(img)\n",
    "img = img.reshape((1,) + img.shape)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.7404578e-04, 5.4284059e-03, 1.3825587e-04, 5.3892395e-04,\n",
       "        9.6923929e-01, 1.1367024e-03, 2.1614123e-04, 5.8221188e-04,\n",
       "        1.3751948e-02, 8.0628862e-04, 8.4050378e-04, 5.0465256e-04,\n",
       "        6.3668517e-04, 5.8059399e-03]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = finetune_model.predict(img)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.   , 0.005, 0.   , 0.001, 0.969, 0.001, 0.   , 0.001, 0.014,\n",
       "        0.001, 0.001, 0.001, 0.001, 0.006]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(pred, decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "['boxer']"
      ],
      "text/plain": [
       "['boxer']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class_indices=np.argmax(pred,axis=1)\n",
    "labels = (train_generator.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "predictions = [labels[k] for k in predicted_class_indices]\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained model and choose two images to explain\n",
    "# model = VGG16(weights='imagenet', include_top=True) ## CHANGE INCLUDE TOP TO FALSE\n",
    "model = tran_model\n",
    "# X,y = shap.datasets.imagenet50()\n",
    "img = load_img('output/val/boxer/1485179194-boxer-dog-portrait.jpg',target_size=(224,224) )\n",
    "img = img_to_array(img)\n",
    "img = img.reshape((1,) + img.shape)\n",
    "to_explain = img\n",
    "X = img\n",
    "\n",
    "# # load the ImageNet class names\n",
    "# url = \"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\"\n",
    "# fname = shap.datasets.cache(url)\n",
    "# with open(fname) as f:\n",
    "#     class_names = json.load(f)\n",
    "class_names = labels\n",
    "\n",
    "# explain how the input to the 7th layer of the model explains the top two classes\n",
    "def map2layer(x, layer):\n",
    "    feed_dict = dict(zip([model.layers[0].input], [preprocess_input(x.copy())]))\n",
    "    return K.get_session().run(model.layers[layer].input, feed_dict)\n",
    "\n",
    "e = shap.GradientExplainer((model.layers[7].input, model.layers[-1].output), map2layer(preprocess_input(X.copy()), 7))\n",
    "shap_values,indexes = e.shap_values(map2layer(to_explain, 7), ranked_outputs=2)\n",
    "\n",
    "# get the names for the classes\n",
    "index_names = np.vectorize(lambda x: class_names[str(x)][1])(indexes)\n",
    "\n",
    "# plot the explanations\n",
    "shap.image_plot(shap_values, to_explain, index_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.get_session().run(model.layers[layer].input, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
